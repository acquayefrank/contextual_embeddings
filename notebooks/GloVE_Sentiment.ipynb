{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GloVE_Sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZrCdEYTLtpyT",
        "2qcap3E2ty-q"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acquayefrank/contextual_embeddings/blob/master/notebooks/GloVE_Sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StVLM4NRqeUI"
      },
      "source": [
        "# A Sentiment Analysis of GloVe embedding\n",
        "\n",
        "This notebook presents a sentiment analysis of Stanford's GloVe embeddings using pretrained neural networks from Allennlp and Huggingface and NLTR's VADER model (link: http://eegilbert.org/papers/icwsm14.vader.hutto.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r06BZc3fuU2H"
      },
      "source": [
        "# Downloading packages and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW-p1ZiQFNjy",
        "outputId": "60ea2a91-231c-4016-a5f9-0a5d4dcd6695"
      },
      "source": [
        "# Downloading embeddings directly from Standford \n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-30 12:27:44--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-06-30 12:27:44--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-06-30 12:27:45--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip          1%[                    ]  14.47M  8.89MB/s               "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfpW94S0sNwC"
      },
      "source": [
        "# Installing allennlp and models\n",
        "!pip install allennlp==1.0.0 allennlp-models==1.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsg220M6JAxA"
      },
      "source": [
        "!pip install scikit-plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGbqk_yRv5wM"
      },
      "source": [
        "from nltk.corpus import words\n",
        "\n",
        "import nltk\n",
        "nltk.download('words')\n",
        "\n",
        "word_list = set(map(str.lower, words.words()))\n",
        "\n",
        "# Total number of words in nltk\n",
        "len(word_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhmPFxaoFUhY"
      },
      "source": [
        "# imports\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        " \n",
        "from tqdm import tqdm_notebook\n",
        " \n",
        "import os, zipfile\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.stats import ttest_ind\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "from scipy.stats import norm\n",
        "from sklearn.neighbors import KernelDensity\n",
        "from sklearn.metrics import auc, roc_auc_score\n",
        "import scikitplot as skplt\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrCdEYTLtpyT"
      },
      "source": [
        "# Extracting Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hfeJIe3FebB"
      },
      "source": [
        "# Extracting embeddings\n",
        "file_name = os.path.abspath('/content/glove.6B.zip') \n",
        "zip_ref = zipfile.ZipFile(file_name) \n",
        "zip_ref.extractall('./')\n",
        "zip_ref.close() \n",
        "os.remove(file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dXx3W6sIAdt"
      },
      "source": [
        "# Making a dict with keys as words and values as list of embedded features\n",
        "f = open('/content/glove.6B.300d.txt')\n",
        "\n",
        "embeddings_index = dict() \n",
        "for line in tqdm_notebook(f):\n",
        "    values = line.split(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfokhxpRQzoD"
      },
      "source": [
        "# Number of words in dict\n",
        "len(embeddings_index.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qcap3E2ty-q"
      },
      "source": [
        "# Downloading models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI3cIqUivZBg"
      },
      "source": [
        "## Pretrained models from allennlp\n",
        "\n",
        "https://demo.allennlp.org/sentiment-analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQcdnmGnoNB8"
      },
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "import allennlp_models.classification\n",
        "\n",
        "# Loading pre-trained models\n",
        "predictor_LSTM = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/basic_stanford_sentiment_treebank-2020.06.09.tar.gz\")\n",
        "predictor_roBERTa = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/sst-roberta-large-2020.06.08.tar.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc2RaMctvjk6"
      },
      "source": [
        "## NLTK pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLiLpH2VR8Jl"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgL241AsvnoV"
      },
      "source": [
        "## HuggingFace default transformer pretrained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byP93b2hexwb"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Allocate a pipeline for sentiment-analysis\n",
        "classifier = pipeline('sentiment-analysis')\n",
        "classifier('man')[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j96ZRbjQur88"
      },
      "source": [
        "# Functions defining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymNu4MeOReGs"
      },
      "source": [
        "def KNW(word, dictionary, n = 1000, metric = 'euclid', order = 'nearest', filter = 'nltk'):\n",
        "    \n",
        "    word = word.lower()\n",
        "    \n",
        "    ''' Here we choose the way to evaluate the distance between words\n",
        "        Sorting all words by distance from input word'''\n",
        "    \n",
        "    if metric == 'euclid':\n",
        "        dist_dict = sorted(dictionary.items(), key=lambda x: np.linalg.norm([dictionary[word] - x[1]]))\n",
        "    if metric == 'manhattan':\n",
        "        dist_dict = sorted(dictionary.items(), key=lambda x: np.linalg.norm([dictionary[word] - x[1]], ord = 1))\n",
        "    if metric == 'cosine':\n",
        "        dist_dict = sorted(dictionary.items(), key=lambda x: cosine(dictionary[word], x[1]))\n",
        "    \n",
        "    ''' Collecting words from sorted dict '''\n",
        "    \n",
        "    words = []\n",
        "    for key, value in dist_dict:\n",
        "\n",
        "    #''' It is necessary to restrict some tokens from dist_dict (with numbers or other symbols within)'''\n",
        "        if filter == None:\n",
        "          if key.isalpha():\n",
        "              words.append(key)\n",
        "\n",
        "    ### Another approach is to use nltk's wordlist to filter inappropriate words from dist_dict\n",
        "        if filter == 'nltk':\n",
        "            if (key in word_list) and key.isalpha():\n",
        "                words.append(key)\n",
        "    \n",
        "    ''' Here we choose what kind of words we want: nearest or farest'''\n",
        "    \n",
        "    if order == 'nearest':\n",
        "        return words[:n]\n",
        "    elif order == 'farest':\n",
        "        return words[-n:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-M0BqF3woV3"
      },
      "source": [
        "def predict_label(word, model = 'LSTM'):\n",
        "    # This function returns the probability of positive class\n",
        "    # If the word was not found in dictionary returns string 'Token not found'\n",
        "    result_LSTM = predictor_LSTM.predict(sentence = word)\n",
        "    result_roBERTa = predictor_roBERTa.predict(sentence = word)\n",
        "\n",
        "    if result_LSTM['token_ids'] == [1]:\n",
        "        result_LSTM['probs'][0] = 'Token not found'\n",
        "    if len(result_roBERTa['tokens']) > 3:\n",
        "        result_roBERTa['probs'][0] = 'Token not found'\n",
        "    \n",
        "    if model == 'LSTM':\n",
        "        return result_LSTM['probs'][0]\n",
        "    if model == 'roBERTa':\n",
        "        return result_roBERTa['probs'][0]\n",
        "    if model == 'VADER':\n",
        "        model = SentimentIntensityAnalyzer()\n",
        "        score = model.polarity_scores(word)\n",
        "        if score['neg'] >= score['neu'] and score['neg'] >= score['pos']:\n",
        "            return 1 - score['neg']\n",
        "        if score['neu'] >= score['pos'] and score['neu'] >= score['neg']:\n",
        "            return 0.5\n",
        "        if score['pos'] >= score['neu'] and score['pos'] >= score['neg']:\n",
        "            return score['pos']\n",
        "    if model == 'hugging':\n",
        "        output = classifier(word)[0]\n",
        "        if output['label'] == 'POSITIVE':\n",
        "            return output['score']\n",
        "        else:\n",
        "            return 1 - output['score']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-0BF0qANt0-"
      },
      "source": [
        "def SentimentDistribution(word, n = 1000, metric = 'cosine', order = 'nearest', filter = None, model = 'LSTM'):\n",
        "\n",
        "    '''This function returns a dictionary. The keys are the nearest words of given one, the values\n",
        "       are probabilities of positive class\n",
        "       If input word does not in model's dictionary of tokens, function returns 'Input token not found'\n",
        "       '''\n",
        "\n",
        "    sentiment_of_input = predict_label(word, model = model)\n",
        "    if sentiment_of_input == 'Token not found':\n",
        "        return 'Input token not found'\n",
        "    words = KNW(word, embeddings_index, n = n, metric = metric, order = order, filter = None)\n",
        "    \n",
        "    dict_of_sentiments = {}\n",
        "    for elem in words:\n",
        "        proba = predict_label(elem, model = model)\n",
        "        if proba != 'Token not found':\n",
        "            dict_of_sentiments[elem] = proba\n",
        "    \n",
        "    return dict_of_sentiments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihx20DSlNR4s"
      },
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import rcParams\n",
        "\n",
        "# figure size in inches\n",
        "rcParams['figure.figsize'] = 7, 15\n",
        "sns.set_style(\"white\")\n",
        "sns.set_context(\"paper\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
        "\n",
        "\n",
        "def DistributionPlotting(word, n = 1000, metric = 'cosine', order = 'nearest', filter = None, model = 'LSTM'):\n",
        "\n",
        "    '''Here we just plot the distribution of sentiments'''\n",
        "\n",
        "    dict_of_sentiments = SentimentDistribution(word, n = 1000, metric = 'cosine', order = 'nearest', filter = None, model = 'LSTM')\n",
        "    if dict_of_sentiments != 'Input token not found':\n",
        "        ax = sns.displot(data=np.round(np.array(list(dict_of_sentiments.values())) * 100), kind=\"kde\", multiple=\"stack\")\n",
        "        ax.set(xlabel=\"Number of observations\", ylabel = \"Probability of Positive class\",  title='Sentiment Distribution')\n",
        "    else:\n",
        "        return 'Input token not found'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc5Isd20y8z1"
      },
      "source": [
        "Description:\n",
        "\n",
        "Function PlotWordPairs plots sentiment distributions of the n nearest words to the word_1 and word_2.  \n",
        "**Arguments:**  \n",
        "* word_1, word_2: Given words  \n",
        "* n: The number of nearest/farest words to analyze\n",
        "* metric: {'cosine', 'euclid', 'manhattan'} default = 'cosine'  \n",
        "  Choose the metric to sort the words by distance from the given one \n",
        "* order = {'nearest', 'farest'} default = 'nearest'  \n",
        "  Choose the order of sorting words: from nearest to farest if order == 'nearest', else from farest to nearest\n",
        "* filter = {None, 'nltk'} default = None  \n",
        "  Filter the embeddings from sorted list of nearest/farest words. Trys to match GloVe embeddings with some words from the given dictionary\n",
        "* model: {'LSTM', 'roBERTa', 'VADER', 'hugging'} default = 'LSTM'  \n",
        "  LSTM and roBERTa models are taken from allennlp https://demo.allennlp.org/sentiment-analysis  \n",
        "  VADER is from nltk sentiment analysis module  \n",
        "  hugging is a default model from https://huggingface.co/transformers/main_classes/pipelines.html#transformers.TextClassificationPipeline\n",
        "\n",
        "* kind: {'hist', 'gaussian', 'cosine', 'tophat', 'exponential'} default = 'hist'  \n",
        "  Select the kind of PairPlot. If hist - draw a histogram, else draw a density curve of chosen smooting type.\n",
        "\n",
        "* compute_auc: \n",
        "\n",
        "* stat_test: {None, 'mannwhitneyu'} default = None\n",
        "  Make a test of difference between distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j_USZj6zX7G"
      },
      "source": [
        "a = np.array([0.1, 0.2, 0.8, 0.9])\n",
        "\n",
        "np.round(a,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z85MVnRh5_Ur"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def PlotWordPairs(word1, word2, kind = 'hist', n = 1000, metric = 'cosine',\n",
        "                  order = 'nearest', filter = None, model = 'LSTM', compute_roc_auc = False, stat_test = None):\n",
        " \n",
        "    word_1_dict = SentimentDistribution(word1, n, metric, order, filter, model)\n",
        "    word_2_dict = SentimentDistribution(word2, n, metric, order, filter, model)\n",
        " \n",
        "    \n",
        "\n",
        "    if word_1_dict != 'Input token not found' and word_2_dict != 'Input token not found':\n",
        "        \n",
        "        data_1 = list(word_1_dict.values())\n",
        "        data_2 = list(word_2_dict.values())\n",
        "\n",
        "        sorted_data_1 = np.sort(data_1)\n",
        "        sorted_data_2 = np.sort(data_2)\n",
        "\n",
        "        if compute_roc_auc == True:\n",
        "\n",
        "            auc_1 = list(zip(data_1, np.ones(len(data_1))))\n",
        "            auc_2 = list(zip(data_2, np.zeros(len(data_2))))\n",
        "\n",
        "            total_auc_data = auc_1 + auc_2\n",
        "            total_auc_data = sorted(total_auc_data, key=lambda item: item[0])\n",
        "\n",
        "            total_auc_data = np.array(total_auc_data)\n",
        "\n",
        "            y_score, y_true = total_auc_data[:, 0], total_auc_data[:, 1]\n",
        "            \n",
        "            auc = roc_auc_score(y_true, y_score)\n",
        "\n",
        "            fpr, tpr, _ = metrics.roc_curve(y_true, y_score)\n",
        "\n",
        "            MW = mannwhitneyu(sorted_data_1, sorted_data_2)\n",
        "            \n",
        "            plt.figure(figsize = (10, 8))\n",
        "            plt.plot(fpr, tpr, label=f\"AUC={round(auc, 2)} , Mann Whitney p_value= {MW[1]}\")\n",
        "            plt.xlabel('FPR')\n",
        "            plt.ylabel('TPR')\n",
        "            plt.title(f\"AUC of {word1} vs {word2} \")\n",
        "            plt.legend()\n",
        "            plt.grid()\n",
        "            plt.show() \n",
        "            \n",
        "            print()\n",
        "            print()\n",
        "            print()\n",
        "\n",
        "\n",
        "\n",
        "        if kind == 'hist':\n",
        "            plt.figure(figsize = (10, 15))\n",
        "            data = {\n",
        "                word1: np.round(np.array(data_1), 2),\n",
        "                word2: np.round(np.array(data_2), 2)\n",
        "            } \n",
        "    \n",
        "            df = pd.DataFrame(data)\n",
        "\n",
        "            sns.histplot(data=df, x=word1, color=\"skyblue\", label=word1, kde=True, bins=10)\n",
        "            sns.histplot(data=df, x=word2, color=\"red\", label=word2, kde=True, bins=10)\n",
        "            \n",
        "            plt.xlabel('Probability of Positive Class')\n",
        "            plt.ylabel('Number of occurances of probability')\n",
        "            plt.title(f\"Sentiment Distribution of {word1} vs {word2} \")\n",
        "            plt.legend() \n",
        "            plt.show()\n",
        "           \n",
        "        \n",
        "        else:\n",
        "\n",
        "            _, bins1, _ = plt.hist(data_1, density=1, alpha=0.5, bins=\"auto\")\n",
        "            _, bins2, _ = plt.hist(data_2, density=1, alpha=0.5, bins=\"auto\")\n",
        "            plt.close()\n",
        "\n",
        " \n",
        "            if kind == 'gaussian':\n",
        "                #\n",
        "                kde_1 = KernelDensity(bandwidth=0.01, kernel='gaussian')\n",
        "                kde_1.fit(np.array(data_1)[:, None])\n",
        "                kde_2 = KernelDensity(bandwidth=0.01, kernel='gaussian')\n",
        "                kde_2.fit(np.array(data_2)[:, None])\n",
        "\n",
        "            if kind == 'cosine':\n",
        "                #\n",
        "                kde_1 = KernelDensity(bandwidth=0.01, kernel='cosine')\n",
        "                kde_1.fit(np.array(data_1)[:, None])\n",
        "                kde_2 = KernelDensity(bandwidth=0.01, kernel='cosine')\n",
        "                kde_2.fit(np.array(data_2)[:, None])\n",
        "            \n",
        "            if kind == 'tophat':\n",
        "                #\n",
        "                kde_1 = KernelDensity(bandwidth=0.01, kernel='tophat')\n",
        "                kde_1.fit(np.array(data_1)[:, None])\n",
        "                kde_2 = KernelDensity(bandwidth=0.01, kernel='tophat')\n",
        "                kde_2.fit(np.array(data_2)[:, None])\n",
        "\n",
        "            if kind == 'exponential':\n",
        "                #\n",
        "                kde_1 = KernelDensity(bandwidth=0.01, kernel='exponential')\n",
        "                kde_1.fit(np.array(data_1)[:, None])\n",
        "                kde_2 = KernelDensity(bandwidth=0.01, kernel='exponential')\n",
        "                kde_2.fit(np.array(data_2)[:, None])\n",
        "\n",
        "\n",
        "            \n",
        "            logprob_1 = kde_1.score_samples(bins1[:, None])\n",
        "            logprob_2 = kde_2.score_samples(bins2[:, None])\n",
        "                \n",
        "            plt.figure(figsize = (10, 7))\n",
        "            plt.fill_between(np.linspace(0, 1, len(bins1)), np.exp(logprob_1), alpha=0.5,\n",
        "                             label = word1 + '; Total number of neighbours ' + str(len(data_1)))\n",
        "            plt.fill_between(np.linspace(0, 1, len(bins2)), np.exp(logprob_2), alpha=0.5,\n",
        "                             label = word2 + '; Total number of neighbours ' + str(len(data_2)))\n",
        "            plt.plot(data_1, np.full_like(data_1, -0.01), '|k', markeredgewidth=5, color='green', alpha = 0.5)\n",
        "            plt.plot(data_2, np.full_like(data_2, -0.01), '|k', markeredgewidth=5, color='black', alpha = 0.5)             \n",
        "                \n",
        "            \n",
        "            plt.title('Sentiment distribution of ' + word1.upper() + ' and ' + word2.upper())\n",
        "            plt.xlabel('Probability of Positive class')\n",
        "            plt.legend(loc = 9)\n",
        "            plt.grid()\n",
        "            plt.show()    \n",
        "\n",
        "                \n",
        "\n",
        " \n",
        "    \n",
        "    else:\n",
        "        return 'Input token not found'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OORXk0QNvQJi"
      },
      "source": [
        "# Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwRWLDotKy6L"
      },
      "source": [
        "PlotWordPairs('Demon', 'Angel', model = 'hugging', kind = 'hist', compute_roc_auc=True, stat_test='mannwhitneyu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woh6Qhh18wcx"
      },
      "source": [
        "PlotWordPairs('China', 'Russia', model = 'hugging', kind = 'hist', compute_roc_auc=True, stat_test='mannwhitneyu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jkS178HyeZH"
      },
      "source": [
        "PlotWordPairs('US', 'Russia', model = 'hugging', kind = 'hist', compute_roc_auc=True, stat_test='mannwhitneyu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFij9BNs5-mP"
      },
      "source": [
        "PlotWordPairs('Cat', 'Dog', model = 'hugging', kind = 'hist', stat_test='mannwhitneyu', compute_roc_auc=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gENqeyjoTR-r"
      },
      "source": [
        "PlotWordPairs('China', 'Russia', model = 'hugging', kind = 'hist', compute_roc_auc=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbbC2A3oTT72"
      },
      "source": [
        "PlotWordPairs('China', 'US', n = 1000, model = 'roBERTa', kind = 'hist', stat_test='mannwhitneyu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bzUSJ-xT5kx"
      },
      "source": [
        "PlotWordPairs('White', 'Black', model = 'LSTM', kind = 'hist')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7WDpiacT_w9"
      },
      "source": [
        "PlotWordPairs('Man', 'Woman', model = 'VADER', kind = 'hist', compute_roc_auc=True, stat_test='mannwhitneyu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYdegV0TUvh5"
      },
      "source": [
        "PlotWordPairs('White', 'Black', n =1000, model = 'LSTM', kind = 'hist')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvjbqcQzWpEx"
      },
      "source": [
        "PlotWordPairs('White', 'Black', n =1000, model = 'hugging', kind = 'hist', compute_roc_auc=True, stat_test='mannwhitneyu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5tyIR-1cV8B"
      },
      "source": [
        "PlotWordPairs('American', 'Russian', n =1000, model = 'hugging', kind = 'hist', compute_roc_auc=True, stat_test='mannwhitneyu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCGfJi8XwFSp"
      },
      "source": [
        "PlotWordPairs('African', 'European', n =1000, model = 'hugging', kind = 'hist', compute_roc_auc=True, stat_test='mannwhitneyu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEGMBFvmzQ-8"
      },
      "source": [
        "PlotWordPairs('White', 'Black', model = 'LSTM', kind = 'hist')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2im_vqay1PSi"
      },
      "source": [
        "PlotWordPairs('Green', 'Nuclear', n =1000, model = 'hugging', kind = 'hist', compute_roc_auc=True, stat_test='mannwhitneyu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EQQxQt41gTK"
      },
      "source": [
        "PlotWordPairs('Ghana', 'USA', n =1000, model = 'hugging', kind = 'hist', compute_roc_auc=True, stat_test='mannwhitneyu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaY3TD7i1q3l"
      },
      "source": [
        "PlotWordPairs('Obama', 'Trump', n =1000, model = 'hugging', kind = 'hist', compute_roc_auc=True, stat_test='mannwhitneyu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuq-4m4oWL9Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}